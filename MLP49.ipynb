{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadaseshel/MLP/blob/main/MLP49.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center><b>Bio-intelligent Algorithms</></center></h1>\n",
        "<h3><center>Exercise 1</center></h3>"
      ],
      "metadata": {
        "id": "Aqk3DEDJpJme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation of the exercise consists of several parts:\n",
        "1. Getting data (train, validation, test and the trained model) from google drive.\n",
        "2. The realization of the model, and the training of the model in the main function.\n",
        "3. Calculation of the accuracy percentage of validation.\n",
        "4. The model prediction for the test set.\n",
        "5. Converting data from csv files to pickle."
      ],
      "metadata": {
        "id": "nD5hiWkEqIt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instructions for performing inference on the test set:** <br>\n",
        "Run cell at section 1 (Getting data from google drive), then run cell at section 4 (The model prediction for the test set)\n",
        "\n",
        "#Instructions for performing train:** <br>\n",
        "Run cell at section 1 (Getting data from google drive), then run cell at section 2 (The realization of the model, and the training of the model in the main function).\n"
      ],
      "metadata": {
        "id": "dz1VNUxOukwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Getting data from google drive.\n",
        "In this part we get the data: train, validation and test from the google drive.<br>\n",
        "We converted the data into pickle files and used them.<br>\n",
        "In addition, we get the model we have already trained and saved as a pickle, the model name is best_model0.487.pickle."
      ],
      "metadata": {
        "id": "AY92xuRwlmI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEsj8mLYyAY2",
        "outputId": "8cd5fa5c-1fa2-42f4-863b-4f170e741c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dpcyX8A4qNVvvtSvFrjg4PF9TvLp0iCe\n",
            "To: /content/validation.pickle\n",
            "100% 24.6M/24.6M [00:00<00:00, 103MB/s] \n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CG3hKEbt7sX6D7Zn8cwb9OJ-Ij8u-qL4\n",
            "To: /content/train.pickle\n",
            "100% 197M/197M [00:01<00:00, 98.4MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Wcpf-ZPzqtccTp2Ry_6TWcRydth4n8l5\n",
            "To: /content/test.pickle\n",
            "100% 24.6M/24.6M [00:00<00:00, 29.4MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IT7zh9BBk-UMWE0PQlYyg_jlbhdmatco\n",
            "To: /content/best_model0.487.pickle\n",
            "100% 53.0M/53.0M [00:01<00:00, 36.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1dpcyX8A4qNVvvtSvFrjg4PF9TvLp0iCe\n",
        "!gdown --id 1CG3hKEbt7sX6D7Zn8cwb9OJ-Ij8u-qL4\n",
        "!gdown --id 1Wcpf-ZPzqtccTp2Ry_6TWcRydth4n8l5\n",
        "!gdown --id 1IT7zh9BBk-UMWE0PQlYyg_jlbhdmatco\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "# Define the device for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def read_train_from_pickle(file_name=\"./train.pickle\"):\n",
        "    with open(file_name, 'rb') as handle:\n",
        "        (x, y) = pickle.load(handle)\n",
        "    return x, y\n",
        "\n",
        "def read_validation_from_pickle(file_name=\"./validation.pickle\"):\n",
        "    with open(file_name, 'rb') as handle:\n",
        "        (x, y) = pickle.load(handle)\n",
        "    return x, y\n",
        "\n",
        "def read_test_from_pickle(file_name=\"./test.pickle\"):\n",
        "    with open(file_name, 'rb') as handle:\n",
        "        x = pickle.load(handle)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Running Multi layer perceptron with relu activation function.\n",
        "\n",
        "**The architecture of the model:** <br>\n",
        "Layers: 3072 -> 550 -> 298 -> 10<br>\n",
        "Learning rate decay every 20 epochs by factor of 2,\n",
        "initialization learning rate 0.001 <br>\n",
        "\n",
        "<br>\n",
        "Saving the weights of the model if it exceed 47% accuracy on validation."
      ],
      "metadata": {
        "id": "9K77n9GnlyKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def xavier_init(in_dim, out_dim):\n",
        "    xavier_stddev = np.sqrt(2.0 / (in_dim + out_dim))\n",
        "    return np.random.normal(0.0, xavier_stddev, (out_dim, out_dim))\n",
        "\n",
        "class SoftmaxLayer:\n",
        "    def __init__(self):\n",
        "        self.probs = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure numerical stability by subtracting the maximum value\n",
        "        shifted_x = x - np.max(x, axis=0)\n",
        "\n",
        "        # Exponentiate the shifted values\n",
        "        exp_x = np.exp(shifted_x)\n",
        "\n",
        "        # Compute the softmax probabilities\n",
        "        self.probs = exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "        return self.probs\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        num_samples = grad_output.shape[1]\n",
        "\n",
        "        # Compute the gradient of the softmax function\n",
        "        grad_input = grad_output - np.sum(self.probs * grad_output, axis=0)\n",
        "        grad_input /= num_samples\n",
        "\n",
        "        return grad_input, None, None\n",
        "\n",
        "\n",
        "class LinearLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size) * 0.01\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.dot(self.weights, x) + self.biases\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = np.dot(self.weights.T, grad_output)\n",
        "        grad_weights = np.dot(grad_output, self.x.T)\n",
        "        grad_biases = np.sum(grad_output, axis=1, keepdims=True)\n",
        "        return grad_input, grad_weights, grad_biases\n",
        "\n",
        "\n",
        "class ReLULayer:\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = grad_output * (self.x > 0)\n",
        "        return grad_input, None, None\n",
        "\n",
        "def forward(x,layers ):\n",
        "    for layer in layers:\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        for size in hidden_sizes:\n",
        "            self.layers.append(LinearLayer(prev_size, size))\n",
        "            self.layers.append(ReLULayer())\n",
        "            prev_size = size\n",
        "\n",
        "        self.layers.append(LinearLayer(prev_size, output_size))\n",
        "        # self.layers.append(SoftmaxLayer())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad_output, learning_rate,epoch):\n",
        "        for layer in reversed(self.layers):\n",
        "            grad_output, grad_weights, grad_biases = layer.backward(grad_output)\n",
        "\n",
        "            if grad_weights is not None and grad_biases is not None:\n",
        "              # Compute the updated learning rate based on the epoch\n",
        "                decay_factor = 0.5  # Factor by which the learning rate will decay\n",
        "                decay_epochs = 20  # Number of epochs after which the learning rate will decay\n",
        "\n",
        "                # Compute the updated learning rate\n",
        "                lr = learning_rate * (decay_factor ** (epoch // decay_epochs))\n",
        "\n",
        "                # Update weights and biases\n",
        "                layer.weights -= lr * grad_weights\n",
        "                layer.biases -= lr * grad_biases\n",
        "\n",
        "        return grad_output\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, batch_size, learning_rate, epochs, validation_freq):\n",
        "        num_samples = X_train.shape[0]\n",
        "        best_accuracy = 0\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                batch_X = X_train[i:i + batch_size]\n",
        "                batch_y = y_train[i:i + batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.forward(batch_X.T)\n",
        "\n",
        "                # Backward pass\n",
        "                grad_output = 2 * (outputs - batch_y.T)\n",
        "                self.backward(grad_output, learning_rate,epoch)\n",
        "            # Compute accuracy on train and validation data\n",
        "            train_outputs = self.forward(X_train.T)\n",
        "            train_accuracy = np.mean(np.argmax(train_outputs, axis=0) == np.argmax(y_train.T, axis=0))\n",
        "\n",
        "            val_outputs = self.forward(X_val.T)\n",
        "            val_accuracy = np.mean(np.argmax(val_outputs, axis=0) == np.argmax(y_val.T, axis=0))\n",
        "            print(\n",
        "                f\"Epoch {epoch + 1}/{epochs} - Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "            if val_accuracy > best_accuracy and val_accuracy > 0.47:\n",
        "                weights = 'best_model' + str(val_accuracy) + '.pickle'\n",
        "                best_accuracy = val_accuracy\n",
        "                print(val_accuracy)\n",
        "                with open(weights, 'wb') as f:\n",
        "                    pickle.dump(self.layers, f)\n",
        "\n",
        "def main():\n",
        "    X_train, y_train = read_train_from_pickle()\n",
        "    X_test, y_test = read_validation_from_pickle()\n",
        "    # One-hot encode the target labels\n",
        "    num_classes = 10\n",
        "    y_train = np.eye(num_classes)[y_train]\n",
        "    y_test = np.eye(num_classes)[y_test]\n",
        "\n",
        "    # Define the MLP architecture\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_sizes = [550, 298]\n",
        "    output_size = num_classes\n",
        "    # mlp = MLP(input_size, hidden_sizes, output_size)\n",
        "    mlp = MLP(input_size, hidden_sizes, output_size)\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.001\n",
        "    epochs = 80\n",
        "    validation_freq = 1\n",
        "\n",
        "    # Train the MLP model\n",
        "    mlp.train(X_train, y_train, X_test, y_test, batch_size, learning_rate, epochs, validation_freq)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "nIZzQ0fWyUzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d694cf-7e54-49ab-97e2-ebbba87eb798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80 - Train Accuracy: 0.2295, Validation Accuracy: 0.2190\n",
            "Epoch 2/80 - Train Accuracy: 0.2751, Validation Accuracy: 0.2690\n",
            "Epoch 3/80 - Train Accuracy: 0.3035, Validation Accuracy: 0.3130\n",
            "Epoch 4/80 - Train Accuracy: 0.3294, Validation Accuracy: 0.3450\n",
            "Epoch 5/80 - Train Accuracy: 0.3454, Validation Accuracy: 0.3430\n",
            "Epoch 6/80 - Train Accuracy: 0.3633, Validation Accuracy: 0.3620\n",
            "Epoch 7/80 - Train Accuracy: 0.3772, Validation Accuracy: 0.3690\n",
            "Epoch 8/80 - Train Accuracy: 0.3854, Validation Accuracy: 0.3840\n",
            "Epoch 9/80 - Train Accuracy: 0.3832, Validation Accuracy: 0.3800\n",
            "Epoch 10/80 - Train Accuracy: 0.3887, Validation Accuracy: 0.3700\n",
            "Epoch 11/80 - Train Accuracy: 0.3865, Validation Accuracy: 0.3780\n",
            "Epoch 12/80 - Train Accuracy: 0.3991, Validation Accuracy: 0.3800\n",
            "Epoch 13/80 - Train Accuracy: 0.4054, Validation Accuracy: 0.3870\n",
            "Epoch 14/80 - Train Accuracy: 0.3996, Validation Accuracy: 0.3940\n",
            "Epoch 15/80 - Train Accuracy: 0.3982, Validation Accuracy: 0.3870\n",
            "Epoch 16/80 - Train Accuracy: 0.4143, Validation Accuracy: 0.3970\n",
            "Epoch 17/80 - Train Accuracy: 0.4244, Validation Accuracy: 0.4000\n",
            "Epoch 18/80 - Train Accuracy: 0.3786, Validation Accuracy: 0.3720\n",
            "Epoch 19/80 - Train Accuracy: 0.4353, Validation Accuracy: 0.3980\n",
            "Epoch 20/80 - Train Accuracy: 0.4319, Validation Accuracy: 0.4030\n",
            "Epoch 21/80 - Train Accuracy: 0.4884, Validation Accuracy: 0.4480\n",
            "Epoch 22/80 - Train Accuracy: 0.4931, Validation Accuracy: 0.4420\n",
            "Epoch 23/80 - Train Accuracy: 0.4935, Validation Accuracy: 0.4360\n",
            "Epoch 24/80 - Train Accuracy: 0.5008, Validation Accuracy: 0.4460\n",
            "Epoch 25/80 - Train Accuracy: 0.4988, Validation Accuracy: 0.4430\n",
            "Epoch 26/80 - Train Accuracy: 0.5175, Validation Accuracy: 0.4480\n",
            "Epoch 27/80 - Train Accuracy: 0.5205, Validation Accuracy: 0.4430\n",
            "Epoch 28/80 - Train Accuracy: 0.5002, Validation Accuracy: 0.4280\n",
            "Epoch 29/80 - Train Accuracy: 0.5215, Validation Accuracy: 0.4490\n",
            "Epoch 30/80 - Train Accuracy: 0.5156, Validation Accuracy: 0.4430\n",
            "Epoch 31/80 - Train Accuracy: 0.5189, Validation Accuracy: 0.4480\n",
            "Epoch 32/80 - Train Accuracy: 0.5249, Validation Accuracy: 0.4450\n",
            "Epoch 33/80 - Train Accuracy: 0.5259, Validation Accuracy: 0.4390\n",
            "Epoch 34/80 - Train Accuracy: 0.5288, Validation Accuracy: 0.4440\n",
            "Epoch 35/80 - Train Accuracy: 0.5460, Validation Accuracy: 0.4410\n",
            "Epoch 36/80 - Train Accuracy: 0.5120, Validation Accuracy: 0.4220\n",
            "Epoch 37/80 - Train Accuracy: 0.5575, Validation Accuracy: 0.4490\n",
            "Epoch 38/80 - Train Accuracy: 0.5457, Validation Accuracy: 0.4410\n",
            "Epoch 39/80 - Train Accuracy: 0.5561, Validation Accuracy: 0.4450\n",
            "Epoch 40/80 - Train Accuracy: 0.5737, Validation Accuracy: 0.4570\n",
            "Epoch 41/80 - Train Accuracy: 0.6078, Validation Accuracy: 0.4690\n",
            "Epoch 42/80 - Train Accuracy: 0.6130, Validation Accuracy: 0.4690\n",
            "Epoch 43/80 - Train Accuracy: 0.6174, Validation Accuracy: 0.4680\n",
            "Epoch 44/80 - Train Accuracy: 0.6240, Validation Accuracy: 0.4740\n",
            "0.474\n",
            "Epoch 45/80 - Train Accuracy: 0.6275, Validation Accuracy: 0.4730\n",
            "Epoch 46/80 - Train Accuracy: 0.6322, Validation Accuracy: 0.4700\n",
            "Epoch 47/80 - Train Accuracy: 0.6345, Validation Accuracy: 0.4700\n",
            "Epoch 48/80 - Train Accuracy: 0.6389, Validation Accuracy: 0.4710\n",
            "Epoch 49/80 - Train Accuracy: 0.6440, Validation Accuracy: 0.4710\n",
            "Epoch 50/80 - Train Accuracy: 0.6482, Validation Accuracy: 0.4680\n",
            "Epoch 51/80 - Train Accuracy: 0.6452, Validation Accuracy: 0.4740\n",
            "Epoch 52/80 - Train Accuracy: 0.6550, Validation Accuracy: 0.4750\n",
            "0.475\n",
            "Epoch 53/80 - Train Accuracy: 0.6570, Validation Accuracy: 0.4720\n",
            "Epoch 54/80 - Train Accuracy: 0.6580, Validation Accuracy: 0.4680\n",
            "Epoch 55/80 - Train Accuracy: 0.6643, Validation Accuracy: 0.4630\n",
            "Epoch 56/80 - Train Accuracy: 0.6634, Validation Accuracy: 0.4710\n",
            "Epoch 57/80 - Train Accuracy: 0.6669, Validation Accuracy: 0.4610\n",
            "Epoch 58/80 - Train Accuracy: 0.6680, Validation Accuracy: 0.4690\n",
            "Epoch 59/80 - Train Accuracy: 0.6736, Validation Accuracy: 0.4630\n",
            "Epoch 60/80 - Train Accuracy: 0.6706, Validation Accuracy: 0.4580\n",
            "Epoch 61/80 - Train Accuracy: 0.7073, Validation Accuracy: 0.4740\n",
            "Epoch 62/80 - Train Accuracy: 0.7113, Validation Accuracy: 0.4760\n",
            "0.476\n",
            "Epoch 63/80 - Train Accuracy: 0.7134, Validation Accuracy: 0.4760\n",
            "Epoch 64/80 - Train Accuracy: 0.7163, Validation Accuracy: 0.4730\n",
            "Epoch 65/80 - Train Accuracy: 0.7201, Validation Accuracy: 0.4710\n",
            "Epoch 66/80 - Train Accuracy: 0.7224, Validation Accuracy: 0.4740\n",
            "Epoch 67/80 - Train Accuracy: 0.7256, Validation Accuracy: 0.4740\n",
            "Epoch 68/80 - Train Accuracy: 0.7276, Validation Accuracy: 0.4730\n",
            "Epoch 69/80 - Train Accuracy: 0.7296, Validation Accuracy: 0.4700\n",
            "Epoch 70/80 - Train Accuracy: 0.7320, Validation Accuracy: 0.4720\n",
            "Epoch 71/80 - Train Accuracy: 0.7345, Validation Accuracy: 0.4740\n",
            "Epoch 72/80 - Train Accuracy: 0.7376, Validation Accuracy: 0.4760\n",
            "Epoch 73/80 - Train Accuracy: 0.7409, Validation Accuracy: 0.4730\n",
            "Epoch 74/80 - Train Accuracy: 0.7429, Validation Accuracy: 0.4780\n",
            "0.478\n",
            "Epoch 75/80 - Train Accuracy: 0.7450, Validation Accuracy: 0.4790\n",
            "0.479\n",
            "Epoch 76/80 - Train Accuracy: 0.7486, Validation Accuracy: 0.4750\n",
            "Epoch 77/80 - Train Accuracy: 0.7496, Validation Accuracy: 0.4820\n",
            "0.482\n",
            "Epoch 78/80 - Train Accuracy: 0.7514, Validation Accuracy: 0.4770\n",
            "Epoch 79/80 - Train Accuracy: 0.7555, Validation Accuracy: 0.4800\n",
            "Epoch 80/80 - Train Accuracy: 0.7562, Validation Accuracy: 0.4760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Calculation of the accuracy percentage of validation.\n",
        "By running this cell you can view the percent accuracy of the model you trained on validation."
      ],
      "metadata": {
        "id": "fUiMTjV5s0kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size) * 0.01\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.dot(self.weights, x) + self.biases\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = np.dot(self.weights.T, grad_output)\n",
        "        grad_weights = np.dot(grad_output, self.x.T)\n",
        "        grad_biases = np.sum(grad_output, axis=1, keepdims=True)\n",
        "        return grad_input, grad_weights, grad_biases\n",
        "\n",
        "class ReLULayer:\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = grad_output * (self.x > 0)\n",
        "        return grad_input, None, None\n",
        "\n",
        "def forward(x,layers ):\n",
        "    for layer in layers:\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "with open('best_model0.487.pickle', 'rb') as handle:\n",
        "        weights = pickle.load(handle)\n",
        "\n",
        "X_val,y_val = read_validation_from_pickle()\n",
        "y_val = np.eye(10)[y_val]\n",
        "\n",
        "val_outputs = forward(X_val.T,weights)\n",
        "val_accuracy = np.mean(np.argmax(val_outputs, axis=0) == np.argmax(y_val.T, axis=0))\n",
        "print(val_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6eoZy1_s06O",
        "outputId": "e0e7ac19-c775-432e-9b07-b0ccf0711789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. The model prediction for the test set.\n",
        "If you would like only to infer on our weights run the following cell - after getting the data from the first cell (In section 1)."
      ],
      "metadata": {
        "id": "3iqn6aRtmjYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size) * 0.01\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.dot(self.weights, x) + self.biases\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = np.dot(self.weights.T, grad_output)\n",
        "        grad_weights = np.dot(grad_output, self.x.T)\n",
        "        grad_biases = np.sum(grad_output, axis=1, keepdims=True)\n",
        "        return grad_input, grad_weights, grad_biases\n",
        "\n",
        "class ReLULayer:\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = grad_output * (self.x > 0)\n",
        "        return grad_input, None, None\n",
        "\n",
        "def forward(x,layers ):\n",
        "    for layer in layers:\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "with open('best_model0.487.pickle', 'rb') as handle:\n",
        "        weights = pickle.load(handle)\n",
        "X_test = read_test_from_pickle()\n",
        "test_outputs = forward(X_test.T,weights)\n",
        "test_outputs = test_outputs.T\n",
        "results = [np.argmax(x,axis=0) + 1 for x in test_outputs]\n",
        "output = '\\n'.join(str(x) for x in results)\n",
        "print(output)\n",
        "with open('output.txt', 'w') as f:\n",
        "    f.write(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFE7icwQXsfF",
        "outputId": "0baf82b8-809e-473a-aace-630b55dd4e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "8\n",
            "10\n",
            "7\n",
            "3\n",
            "6\n",
            "3\n",
            "1\n",
            "10\n",
            "1\n",
            "10\n",
            "1\n",
            "6\n",
            "7\n",
            "9\n",
            "9\n",
            "1\n",
            "5\n",
            "10\n",
            "3\n",
            "9\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "8\n",
            "1\n",
            "1\n",
            "6\n",
            "1\n",
            "2\n",
            "6\n",
            "1\n",
            "8\n",
            "9\n",
            "1\n",
            "10\n",
            "7\n",
            "2\n",
            "7\n",
            "1\n",
            "6\n",
            "6\n",
            "3\n",
            "2\n",
            "10\n",
            "3\n",
            "9\n",
            "6\n",
            "10\n",
            "9\n",
            "1\n",
            "10\n",
            "2\n",
            "7\n",
            "4\n",
            "1\n",
            "9\n",
            "9\n",
            "6\n",
            "4\n",
            "8\n",
            "2\n",
            "8\n",
            "9\n",
            "1\n",
            "1\n",
            "4\n",
            "9\n",
            "3\n",
            "6\n",
            "10\n",
            "10\n",
            "8\n",
            "9\n",
            "10\n",
            "5\n",
            "1\n",
            "2\n",
            "6\n",
            "10\n",
            "1\n",
            "6\n",
            "1\n",
            "6\n",
            "6\n",
            "7\n",
            "10\n",
            "7\n",
            "9\n",
            "7\n",
            "7\n",
            "6\n",
            "1\n",
            "2\n",
            "4\n",
            "9\n",
            "5\n",
            "1\n",
            "10\n",
            "4\n",
            "9\n",
            "1\n",
            "10\n",
            "6\n",
            "1\n",
            "4\n",
            "6\n",
            "7\n",
            "4\n",
            "10\n",
            "3\n",
            "6\n",
            "10\n",
            "1\n",
            "1\n",
            "2\n",
            "5\n",
            "8\n",
            "10\n",
            "8\n",
            "4\n",
            "9\n",
            "9\n",
            "10\n",
            "2\n",
            "9\n",
            "3\n",
            "9\n",
            "1\n",
            "9\n",
            "2\n",
            "10\n",
            "8\n",
            "5\n",
            "5\n",
            "3\n",
            "3\n",
            "7\n",
            "6\n",
            "1\n",
            "5\n",
            "2\n",
            "2\n",
            "5\n",
            "7\n",
            "7\n",
            "8\n",
            "2\n",
            "4\n",
            "6\n",
            "9\n",
            "1\n",
            "6\n",
            "5\n",
            "4\n",
            "7\n",
            "6\n",
            "6\n",
            "6\n",
            "5\n",
            "8\n",
            "2\n",
            "9\n",
            "3\n",
            "9\n",
            "1\n",
            "3\n",
            "7\n",
            "6\n",
            "9\n",
            "9\n",
            "4\n",
            "9\n",
            "9\n",
            "8\n",
            "6\n",
            "8\n",
            "5\n",
            "7\n",
            "2\n",
            "10\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "6\n",
            "10\n",
            "7\n",
            "7\n",
            "6\n",
            "2\n",
            "10\n",
            "5\n",
            "9\n",
            "1\n",
            "3\n",
            "7\n",
            "10\n",
            "8\n",
            "6\n",
            "2\n",
            "2\n",
            "5\n",
            "8\n",
            "8\n",
            "3\n",
            "7\n",
            "5\n",
            "7\n",
            "6\n",
            "3\n",
            "3\n",
            "8\n",
            "5\n",
            "1\n",
            "4\n",
            "3\n",
            "9\n",
            "5\n",
            "2\n",
            "8\n",
            "4\n",
            "4\n",
            "3\n",
            "10\n",
            "9\n",
            "2\n",
            "8\n",
            "10\n",
            "3\n",
            "9\n",
            "10\n",
            "8\n",
            "8\n",
            "1\n",
            "9\n",
            "10\n",
            "7\n",
            "7\n",
            "5\n",
            "9\n",
            "1\n",
            "5\n",
            "4\n",
            "6\n",
            "8\n",
            "2\n",
            "4\n",
            "1\n",
            "9\n",
            "6\n",
            "9\n",
            "1\n",
            "4\n",
            "7\n",
            "10\n",
            "3\n",
            "8\n",
            "2\n",
            "7\n",
            "5\n",
            "8\n",
            "5\n",
            "4\n",
            "9\n",
            "9\n",
            "4\n",
            "9\n",
            "6\n",
            "3\n",
            "5\n",
            "1\n",
            "7\n",
            "4\n",
            "3\n",
            "1\n",
            "8\n",
            "10\n",
            "10\n",
            "7\n",
            "1\n",
            "9\n",
            "3\n",
            "5\n",
            "9\n",
            "8\n",
            "8\n",
            "10\n",
            "4\n",
            "3\n",
            "2\n",
            "6\n",
            "1\n",
            "3\n",
            "6\n",
            "9\n",
            "3\n",
            "10\n",
            "8\n",
            "2\n",
            "10\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "9\n",
            "7\n",
            "9\n",
            "10\n",
            "10\n",
            "3\n",
            "7\n",
            "9\n",
            "1\n",
            "4\n",
            "10\n",
            "7\n",
            "7\n",
            "7\n",
            "8\n",
            "3\n",
            "8\n",
            "2\n",
            "8\n",
            "5\n",
            "4\n",
            "3\n",
            "7\n",
            "1\n",
            "7\n",
            "1\n",
            "9\n",
            "6\n",
            "6\n",
            "9\n",
            "7\n",
            "10\n",
            "1\n",
            "7\n",
            "2\n",
            "10\n",
            "8\n",
            "4\n",
            "10\n",
            "3\n",
            "1\n",
            "7\n",
            "2\n",
            "4\n",
            "10\n",
            "6\n",
            "10\n",
            "2\n",
            "10\n",
            "10\n",
            "9\n",
            "5\n",
            "3\n",
            "5\n",
            "8\n",
            "4\n",
            "2\n",
            "3\n",
            "2\n",
            "7\n",
            "10\n",
            "4\n",
            "2\n",
            "3\n",
            "8\n",
            "8\n",
            "5\n",
            "3\n",
            "7\n",
            "3\n",
            "10\n",
            "5\n",
            "2\n",
            "10\n",
            "7\n",
            "6\n",
            "8\n",
            "3\n",
            "4\n",
            "3\n",
            "5\n",
            "1\n",
            "8\n",
            "1\n",
            "1\n",
            "3\n",
            "8\n",
            "10\n",
            "8\n",
            "7\n",
            "10\n",
            "1\n",
            "1\n",
            "5\n",
            "4\n",
            "9\n",
            "7\n",
            "7\n",
            "2\n",
            "5\n",
            "5\n",
            "7\n",
            "1\n",
            "2\n",
            "10\n",
            "7\n",
            "9\n",
            "4\n",
            "9\n",
            "9\n",
            "8\n",
            "8\n",
            "7\n",
            "7\n",
            "3\n",
            "6\n",
            "6\n",
            "4\n",
            "3\n",
            "5\n",
            "6\n",
            "3\n",
            "2\n",
            "3\n",
            "8\n",
            "10\n",
            "6\n",
            "7\n",
            "1\n",
            "8\n",
            "3\n",
            "1\n",
            "3\n",
            "3\n",
            "5\n",
            "10\n",
            "6\n",
            "1\n",
            "1\n",
            "9\n",
            "6\n",
            "7\n",
            "4\n",
            "2\n",
            "10\n",
            "7\n",
            "6\n",
            "2\n",
            "4\n",
            "7\n",
            "10\n",
            "5\n",
            "5\n",
            "3\n",
            "6\n",
            "5\n",
            "3\n",
            "2\n",
            "4\n",
            "4\n",
            "8\n",
            "6\n",
            "2\n",
            "10\n",
            "4\n",
            "10\n",
            "8\n",
            "10\n",
            "5\n",
            "4\n",
            "6\n",
            "3\n",
            "1\n",
            "1\n",
            "2\n",
            "7\n",
            "4\n",
            "2\n",
            "1\n",
            "10\n",
            "4\n",
            "8\n",
            "1\n",
            "10\n",
            "3\n",
            "1\n",
            "9\n",
            "5\n",
            "7\n",
            "7\n",
            "6\n",
            "8\n",
            "6\n",
            "7\n",
            "7\n",
            "9\n",
            "9\n",
            "4\n",
            "10\n",
            "6\n",
            "3\n",
            "7\n",
            "9\n",
            "8\n",
            "7\n",
            "5\n",
            "9\n",
            "4\n",
            "7\n",
            "7\n",
            "8\n",
            "4\n",
            "8\n",
            "3\n",
            "6\n",
            "8\n",
            "9\n",
            "3\n",
            "10\n",
            "2\n",
            "1\n",
            "10\n",
            "3\n",
            "9\n",
            "9\n",
            "7\n",
            "2\n",
            "10\n",
            "1\n",
            "3\n",
            "3\n",
            "6\n",
            "4\n",
            "9\n",
            "9\n",
            "3\n",
            "4\n",
            "6\n",
            "7\n",
            "9\n",
            "9\n",
            "5\n",
            "10\n",
            "4\n",
            "6\n",
            "3\n",
            "7\n",
            "10\n",
            "8\n",
            "5\n",
            "9\n",
            "7\n",
            "8\n",
            "1\n",
            "3\n",
            "7\n",
            "7\n",
            "2\n",
            "2\n",
            "4\n",
            "9\n",
            "5\n",
            "6\n",
            "2\n",
            "2\n",
            "9\n",
            "9\n",
            "10\n",
            "2\n",
            "5\n",
            "7\n",
            "6\n",
            "2\n",
            "4\n",
            "2\n",
            "8\n",
            "6\n",
            "3\n",
            "5\n",
            "3\n",
            "9\n",
            "7\n",
            "4\n",
            "6\n",
            "8\n",
            "4\n",
            "10\n",
            "7\n",
            "4\n",
            "6\n",
            "10\n",
            "1\n",
            "8\n",
            "8\n",
            "2\n",
            "4\n",
            "1\n",
            "7\n",
            "9\n",
            "5\n",
            "1\n",
            "3\n",
            "7\n",
            "3\n",
            "5\n",
            "6\n",
            "4\n",
            "5\n",
            "3\n",
            "2\n",
            "4\n",
            "9\n",
            "1\n",
            "10\n",
            "9\n",
            "3\n",
            "6\n",
            "6\n",
            "1\n",
            "2\n",
            "3\n",
            "10\n",
            "7\n",
            "1\n",
            "4\n",
            "3\n",
            "5\n",
            "9\n",
            "4\n",
            "3\n",
            "1\n",
            "1\n",
            "8\n",
            "10\n",
            "7\n",
            "1\n",
            "5\n",
            "1\n",
            "7\n",
            "3\n",
            "7\n",
            "9\n",
            "5\n",
            "3\n",
            "8\n",
            "9\n",
            "7\n",
            "7\n",
            "10\n",
            "9\n",
            "3\n",
            "3\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "7\n",
            "9\n",
            "5\n",
            "10\n",
            "6\n",
            "9\n",
            "10\n",
            "10\n",
            "1\n",
            "5\n",
            "4\n",
            "10\n",
            "5\n",
            "5\n",
            "1\n",
            "10\n",
            "9\n",
            "2\n",
            "10\n",
            "6\n",
            "6\n",
            "3\n",
            "7\n",
            "1\n",
            "9\n",
            "10\n",
            "5\n",
            "3\n",
            "8\n",
            "7\n",
            "4\n",
            "5\n",
            "7\n",
            "2\n",
            "10\n",
            "6\n",
            "2\n",
            "5\n",
            "6\n",
            "3\n",
            "5\n",
            "5\n",
            "7\n",
            "9\n",
            "7\n",
            "4\n",
            "1\n",
            "2\n",
            "9\n",
            "6\n",
            "9\n",
            "3\n",
            "2\n",
            "2\n",
            "6\n",
            "1\n",
            "10\n",
            "9\n",
            "5\n",
            "10\n",
            "3\n",
            "2\n",
            "3\n",
            "6\n",
            "6\n",
            "3\n",
            "1\n",
            "9\n",
            "5\n",
            "3\n",
            "9\n",
            "2\n",
            "4\n",
            "5\n",
            "7\n",
            "2\n",
            "5\n",
            "1\n",
            "1\n",
            "8\n",
            "4\n",
            "8\n",
            "9\n",
            "8\n",
            "8\n",
            "9\n",
            "9\n",
            "7\n",
            "10\n",
            "6\n",
            "9\n",
            "2\n",
            "8\n",
            "8\n",
            "7\n",
            "8\n",
            "2\n",
            "9\n",
            "1\n",
            "7\n",
            "7\n",
            "2\n",
            "10\n",
            "10\n",
            "3\n",
            "9\n",
            "4\n",
            "3\n",
            "1\n",
            "8\n",
            "6\n",
            "8\n",
            "6\n",
            "9\n",
            "4\n",
            "9\n",
            "8\n",
            "1\n",
            "1\n",
            "1\n",
            "10\n",
            "8\n",
            "3\n",
            "9\n",
            "4\n",
            "9\n",
            "6\n",
            "1\n",
            "8\n",
            "2\n",
            "2\n",
            "8\n",
            "4\n",
            "10\n",
            "9\n",
            "10\n",
            "4\n",
            "6\n",
            "9\n",
            "8\n",
            "3\n",
            "1\n",
            "10\n",
            "3\n",
            "9\n",
            "4\n",
            "6\n",
            "10\n",
            "7\n",
            "7\n",
            "1\n",
            "1\n",
            "10\n",
            "8\n",
            "1\n",
            "1\n",
            "7\n",
            "6\n",
            "8\n",
            "7\n",
            "7\n",
            "10\n",
            "7\n",
            "10\n",
            "3\n",
            "1\n",
            "9\n",
            "6\n",
            "10\n",
            "1\n",
            "3\n",
            "9\n",
            "3\n",
            "9\n",
            "4\n",
            "2\n",
            "5\n",
            "10\n",
            "2\n",
            "6\n",
            "9\n",
            "1\n",
            "2\n",
            "7\n",
            "4\n",
            "7\n",
            "8\n",
            "8\n",
            "1\n",
            "1\n",
            "7\n",
            "9\n",
            "3\n",
            "7\n",
            "3\n",
            "1\n",
            "1\n",
            "10\n",
            "9\n",
            "10\n",
            "5\n",
            "3\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "10\n",
            "7\n",
            "10\n",
            "4\n",
            "9\n",
            "6\n",
            "3\n",
            "10\n",
            "7\n",
            "1\n",
            "1\n",
            "2\n",
            "7\n",
            "4\n",
            "10\n",
            "9\n",
            "5\n",
            "6\n",
            "10\n",
            "5\n",
            "10\n",
            "10\n",
            "10\n",
            "6\n",
            "10\n",
            "1\n",
            "3\n",
            "7\n",
            "5\n",
            "2\n",
            "7\n",
            "7\n",
            "9\n",
            "1\n",
            "7\n",
            "1\n",
            "5\n",
            "7\n",
            "4\n",
            "6\n",
            "8\n",
            "9\n",
            "2\n",
            "9\n",
            "6\n",
            "3\n",
            "10\n",
            "9\n",
            "7\n",
            "1\n",
            "8\n",
            "8\n",
            "5\n",
            "4\n",
            "1\n",
            "7\n",
            "6\n",
            "2\n",
            "7\n",
            "1\n",
            "2\n",
            "2\n",
            "8\n",
            "4\n",
            "9\n",
            "3\n",
            "2\n",
            "3\n",
            "1\n",
            "6\n",
            "6\n",
            "5\n",
            "4\n",
            "1\n",
            "5\n",
            "2\n",
            "9\n",
            "10\n",
            "2\n",
            "10\n",
            "7\n",
            "3\n",
            "3\n",
            "5\n",
            "10\n",
            "9\n",
            "7\n",
            "9\n",
            "7\n",
            "9\n",
            "3\n",
            "8\n",
            "3\n",
            "8\n",
            "6\n",
            "3\n",
            "7\n",
            "4\n",
            "7\n",
            "10\n",
            "1\n",
            "6\n",
            "2\n",
            "1\n",
            "8\n",
            "1\n",
            "9\n",
            "9\n",
            "3\n",
            "8\n",
            "9\n",
            "1\n",
            "10\n",
            "2\n",
            "4\n",
            "5\n",
            "1\n",
            "8\n",
            "8\n",
            "9\n",
            "2\n",
            "5\n",
            "5\n",
            "10\n",
            "6\n",
            "3\n",
            "9\n",
            "9\n",
            "8\n",
            "3\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Converting data from csv files to pickle.\n",
        "We have attached here the code that we ran locally on the computer to process the data and convert it to pickle for convenience."
      ],
      "metadata": {
        "id": "XAdJ28vIwBKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pickle\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "def convert_dataframe(df, has_y=True) -> Tuple[List[Tuple], List[Tuple]]:\n",
        "    inputs = list(df.loc[:, df.columns != 0].itertuples(index=False, name=None))\n",
        "    if has_y:\n",
        "        outputs = list(df.loc[:, df.columns == 0].itertuples(index=False, name=None))\n",
        "    else:\n",
        "        outputs = []\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "def to_black_white(inputs):\n",
        "    converted_inputs = []\n",
        "    for x in inputs:\n",
        "        arr = np.array(x)\n",
        "        converted_inputs.append(np.reshape(arr, (32, 32, 3)).mean(axis=2).flatten())\n",
        "    return converted_inputs\n",
        "\n",
        "\n",
        "def read_from_files(file_name=\"../data/train.csv\", pickle_name='train.pickle'):\n",
        "    data_df = pd.read_csv(file_name, header=None)\n",
        "    x, y = convert_dataframe(data_df)\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    y = y - 1\n",
        "    y = y.reshape(len(y), )\n",
        "    with open(pickle_name, 'wb') as handle:\n",
        "        pickle.dump((x, y), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def read_test_files(file_name=\"../data/test.csv\", pickle_name='test.pickle'):\n",
        "    data_df = pd.read_csv(file_name, header=None)\n",
        "    x, _ = convert_dataframe(data_df, False)\n",
        "    x = np.array(x)\n",
        "    with open(pickle_name, 'wb') as handle:\n",
        "        pickle.dump(x, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    return x"
      ],
      "metadata": {
        "id": "x0UifTc-wBl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}